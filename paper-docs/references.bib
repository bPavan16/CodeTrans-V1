@misc{bert2018,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{gpt32020,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{codebert2020,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08155}, 
}

@misc{t52019,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{abstree2023,
      title={Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?}, 
      author={Weisong Sun and Chunrong Fang and Yun Miao and Yudu You and Mengzhe Yuan and Yuchen Chen and Quanjun Zhang and An Guo and Xiang Chen and Yang Liu and Zhenyu Chen},
      year={2023},
      eprint={2312.00413},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2312.00413}, 
}

@misc{lachaux2020,
      title={Unsupervised Translation of Programming Languages}, 
      author={Marie-Anne Lachaux and Baptiste Roziere and Lowik Chanussot and Guillaume Lample},
      year={2020},
      eprint={2006.03511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.03511}, 
}

@misc{katz2019,
      title={Towards Neural Decompilation}, 
      author={Omer Katz and Yuval Olshaker and Yoav Goldberg and Eran Yahav},
      year={2019},
      eprint={1905.08325},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/1905.08325}, 
}

@misc{tai2015,
      title={Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}, 
      author={Kai Sheng Tai and Richard Socher and Christopher D. Manning},
      year={2015},
      eprint={1503.00075},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1503.00075}, 
}

@misc{nmt2019,
      title={Neural Machine Translation: A Review and Survey}, 
      author={Felix Stahlberg},
      year={2020},
      eprint={1912.02047},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1912.02047}, 
}

@misc{chen2018,
      title={Tree-to-tree Neural Networks for Program Translation}, 
      author={Xinyun Chen and Chang Liu and Dawn Song},
      year={2018},
      eprint={1802.03691},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.03691}, 
}

@misc{codet52021,
      title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
      author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
      year={2021},
      eprint={2109.00859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.00859}, 
}

@misc{xlnet2019,
      title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
      author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
      year={2020},
      eprint={1906.08237},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.08237}, 
}

@misc{drissi2018,
      title={Program Language Translation Using a Grammar-Driven Tree-to-Tree Model}, 
      author={Mehdi Drissi and Olivia Watkins and Aditya Khant and Vivaswat Ojha and Pedro Sandoval and Rakia Segev and Eric Weiner and Robert Keller},
      year={2018},
      eprint={1807.01784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.01784}, 
}

@misc{tufano2018,
      title={An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation}, 
      author={Michele Tufano and Cody Watson and Gabriele Bavota and Massimiliano Di Penta and Martin White and Denys Poshyvanyk},
      year={2019},
      eprint={1812.08693},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/1812.08693}, 
}

@misc{xiyu2024,
      title={Transformers are Efficient Compilers, Provably}, 
      author={Xiyu Zhai and Runlong Zhou and Liao Zhang and Simon Shaolei Du},
      year={2024},
      eprint={2410.14706},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2410.14706}, 
}

@misc{pan2023,
      title={SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation}, 
      author={Jialing Pan and Adrien Sad√© and Jin Kim and Eric Soriano and Guillem Sole and Sylvain Flamant},
      year={2023},
      eprint={2310.15539},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.15539}, 
}

@misc{xlcost2022,
      title={XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence}, 
      author={Ming Zhu and Aneesh Jain and Karthik Suresh and Roshan Ravindran and Sindhu Tipirneni and Chandan K. Reddy},
      year={2022},
      eprint={2206.08474},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2206.08474}, 
}

@misc{gemini2023,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu et al.},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{kyle2021,
      title={Understanding How Encoder-Decoder Architectures Attend}, 
      author={Kyle Aitken and Vinay V Ramasesh and Yuan Cao and Niru Maheswaranathan},
      year={2021},
      eprint={2110.15253},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.15253}, 
}

@misc{bart2019,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13461}, 
}
